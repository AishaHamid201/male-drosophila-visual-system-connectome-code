{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\"\"\"\n",
    "This cell does the initial project setup.\n",
    "If you start a new script or notebook, make sure to copy & paste this part.\n",
    "\n",
    "A script with this code uses the location of the `.env` file as the anchor for\n",
    "the whole project (= PROJECT_ROOT). Afterwards, code inside the `src` directory\n",
    "are available for import.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "PROJECT_ROOT = Path(find_dotenv()).parent\n",
    "sys.path.append(str(PROJECT_ROOT.joinpath(\"src\")))\n",
    "print(f\"Project root directory: {PROJECT_ROOT}\")\n",
    "\n",
    "from utils import olc_client\n",
    "c = olc_client.connect(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ol_types import OLTypes\n",
    "olt = OLTypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuprint import fetch_custom\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "def get_con_weights(classification):\n",
    "    \"\"\" \n",
    "    One way to calculate the connections is looking at actual neuron-to-neuron connections.\n",
    "    \"\"\"\n",
    "\n",
    "    neuron_list = olt.get_neuron_list(primary_classification=classification, side='both')\n",
    "    df = pd.DataFrame()\n",
    "    for name, test in neuron_list.iterrows():\n",
    "        cql = f\"\"\"\n",
    "            MATCH (n:Neuron)-[e:ConnectsTo]->(m:Neuron)\n",
    "            WHERE n.instance='{test['instance']}'\n",
    "            WITH apoc.convert.fromJsonMap(e.roiInfo) AS eri\n",
    "              , n\n",
    "              , m\n",
    "              , e.weight AS wgt\n",
    "            WITH \n",
    "                COALESCE(eri['ME(R)']['post'], 0) as me_wgt\n",
    "              , COALESCE(eri['LA(R)']['post'], 0) as la_wgt\n",
    "              , COALESCE(eri['LO(R)']['post'], 0) as lo_wgt\n",
    "              , COALESCE(eri['LOP(R)']['post'], 0) as lop_wgt\n",
    "              , COALESCE(eri['AME(R)']['post'], 0) as ame_wgt\n",
    "              , COALESCE(eri['OL(R)']['post'], 0) as ol_wgt\n",
    "              , wgt\n",
    "              , n\n",
    "              , m\n",
    "              , eri\n",
    "            RETURN \n",
    "                n.bodyId AS bodyId\n",
    "              , n.instance AS instance\n",
    "              , '{classification}' as class\n",
    "              , sum(wgt) as total_weight\n",
    "              , sum(ol_wgt) AS OL_weight\n",
    "              , sum(la_wgt) AS LA_weight\n",
    "              , sum(me_wgt) AS ME_weight\n",
    "              , sum(lop_wgt) AS LOP_weight\n",
    "              , sum(lo_wgt) AS LO_weight\n",
    "              , sum(ame_wgt) AS AME_weight \n",
    "              , toFloat(sum(la_wgt) + sum(me_wgt) + sum(lop_wgt) + sum(lo_wgt) + sum(ame_wgt))/sum(wgt) as five_weight\n",
    "              \"\"\"\n",
    "        named_df = fetch_custom(cql)\n",
    "        try:\n",
    "            df = pd.concat([df, named_df])\n",
    "        except FutureWarning:\n",
    "            print(f\"no results for {test['instance']}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_neu_weights(neuron_list, area):\n",
    "    \"\"\"\n",
    "    Get the connections from the neurons themselves.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neuron_list : pd.DataFrame\n",
    "        must contain a column 'instance', which is used in the query\n",
    "    area : str\n",
    "        name of the classification\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    df : pd.DataFrame\n",
    "        synaptic weights per bodyId\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for name, test in neuron_list.iterrows():\n",
    "        cql = f\"\"\"\n",
    "            MATCH (n:Neuron)\n",
    "            WHERE n.instance='{test['instance']}'\n",
    "            WITH apoc.convert.fromJsonMap(n.roiInfo) AS nri, n\n",
    "            WITH \n",
    "                COALESCE(nri['ME(R)']['post'], 0) as me_post_wgt\n",
    "              , COALESCE(nri['LA(R)']['post'], 0) as la_post_wgt\n",
    "              , COALESCE(nri['LO(R)']['post'], 0) as lo_post_wgt\n",
    "              , COALESCE(nri['LOP(R)']['post'], 0) as lop_post_wgt\n",
    "              , COALESCE(nri['AME(R)']['post'], 0) as ame_post_wgt\n",
    "              , COALESCE(nri['OL(R)']['post'], 0) as ol_post_wgt\n",
    "              , COALESCE(n.post, 0) as post_wgt\n",
    "              , COALESCE(nri['ME(R)']['pre'], 0) as me_wgt\n",
    "              , COALESCE(nri['LA(R)']['pre'], 0) as la_wgt\n",
    "              , COALESCE(nri['LO(R)']['pre'], 0) as lo_wgt\n",
    "              , COALESCE(nri['LOP(R)']['pre'], 0) as lop_wgt\n",
    "              , COALESCE(nri['AME(R)']['pre'], 0) as ame_wgt\n",
    "              , COALESCE(nri['OL(R)']['pre'], 0) as ol_wgt\n",
    "              , COALESCE(n.pre, 0) as pre_wgt\n",
    "              , COALESCE(nri['ME(R)']['downstream'], 0) as me_down_wgt\n",
    "              , COALESCE(nri['LA(R)']['downstream'], 0) as la_down_wgt\n",
    "              , COALESCE(nri['LO(R)']['downstream'], 0) as lo_down_wgt\n",
    "              , COALESCE(nri['LOP(R)']['downstream'], 0) as lop_down_wgt\n",
    "              , COALESCE(nri['AME(R)']['downstream'], 0) as ame_down_wgt\n",
    "              , COALESCE(nri['OL(R)']['downstream'], 0) as ol_down_wgt\n",
    "              , COALESCE(n.downstream, 0) as down_wgt\n",
    "              , n\n",
    "            RETURN \n",
    "                n.bodyId AS bodyId\n",
    "              , n.instance AS instance\n",
    "              , '{area}' as class\n",
    "              , sum(la_post_wgt) as la_post_weight\n",
    "              , sum(me_post_wgt) as me_post_weight\n",
    "              , sum(lop_post_wgt) as lop_post_weight\n",
    "              , sum(lo_post_wgt) as lo_post_weight\n",
    "              , sum(ame_post_wgt) as ame_post_weight\n",
    "              , sum(ol_post_wgt) as ol_post_weight\n",
    "              , sum(post_wgt) as post_weight\n",
    "              , sum(la_wgt) as la_weight\n",
    "              , sum(me_wgt) as me_weight\n",
    "              , sum(lop_wgt) as lop_weight\n",
    "              , sum(lo_wgt) as lo_weight\n",
    "              , sum(ame_wgt) as ame_weight\n",
    "              , sum(ol_wgt) as ol_weight\n",
    "              , sum(pre_wgt) as pre_weight\n",
    "              , sum(la_down_wgt) as la_down_weight\n",
    "              , sum(me_down_wgt) as me_down_weight\n",
    "              , sum(lop_down_wgt) as lop_down_weight\n",
    "              , sum(lo_down_wgt) as lo_down_weight\n",
    "              , sum(ame_down_wgt) as ame_down_weight\n",
    "              , sum(ol_down_wgt) as ol_down_weight\n",
    "              , sum(down_wgt) as down_weight\n",
    "              \"\"\"\n",
    "        named_df = fetch_custom(cql)\n",
    "        try:\n",
    "            df = pd.concat([df, named_df])\n",
    "        except FutureWarning:\n",
    "            print(f\"no results for {test['instance']}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize(df, name):\n",
    "    \"\"\"\n",
    "    Aggregate the connections per bodyId to connections per cell instance and saves it to a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        data frame to be aggregated\n",
    "    name : str\n",
    "        Excel file name to be created inside `results/test`\n",
    "    \"\"\"\n",
    "    out_fn = PROJECT_ROOT / 'results' / 'test' / name\n",
    "    out_fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df\\\n",
    "        .assign(\n",
    "            post_5_perc = lambda row: (row['la_post_weight'] + row['me_post_weight'] + row['lop_post_weight'] + row['lo_post_weight'] + row['ame_post_weight']) / row['post_weight']\n",
    "          , post_5_ol_perc = lambda row: row['ol_post_weight'] / row['post_weight']\n",
    "          , post_ol_perc = lambda row: (row['la_post_weight'] + row['me_post_weight'] + row['lop_post_weight'] + row['lo_post_weight'] + row['ame_post_weight']) / row['ol_post_weight']\n",
    "          , pre_5_perc = lambda row: (row['la_weight'] + row['me_weight'] + row['lop_weight'] + row['lo_weight'] + row['ame_weight']) / row['pre_weight']\n",
    "          , pre_5_ol_perc = lambda row: row['ol_weight'] / row['pre_weight']\n",
    "          , pre_ol_perc = lambda row: (row['la_weight'] + row['me_weight'] + row['lop_weight'] + row['lo_weight'] + row['ame_weight']) / row['ol_weight']\n",
    "          , down_5_perc = lambda row: (row['la_down_weight'] + row['me_down_weight'] + row['lop_down_weight'] + row['lo_down_weight'] + row['ame_down_weight']) / row['down_weight']\n",
    "          , down_5_ol_perc = lambda row: row['ol_down_weight'] / row['down_weight']\n",
    "          , down_ol_perc = lambda row: (row['la_down_weight'] + row['me_down_weight'] + row['lop_down_weight'] + row['lo_down_weight'] + row['ame_down_weight']) / row['ol_down_weight']\n",
    "          , post_5_tresh = lambda row: round((row['post_5_perc'])-.1+.5)\n",
    "          , pre_5_tresh = lambda row: round((row['pre_5_perc'])-.1+.5)\n",
    "          , down_5_tresh = lambda row: round((row['down_5_perc'])-.1+.5)\n",
    "        )\\\n",
    "        .groupby('instance')\\\n",
    "        .agg({\n",
    "            'bodyId': 'size'\n",
    "          , 'class': 'first'\n",
    "          , 'post_5_perc': 'mean'\n",
    "          , 'post_5_tresh': 'sum'\n",
    "          , 'pre_5_perc': 'mean'\n",
    "          , 'pre_5_tresh': 'sum'\n",
    "          , 'down_5_perc': 'mean'\n",
    "          , 'down_5_tresh': 'sum'\n",
    "        })\\\n",
    "        .rename(columns={\n",
    "            'bodyId': 'num. cells of type'\n",
    "          , 'post_5_perc': '% input connections across all cells of type in the 5 OL neuropils'\n",
    "          , 'post_5_tresh': '# cells of type above 10% input connections in the 5 OL neuropils'\n",
    "          , 'down_5_perc': '% output connections across all cells of type in the 5 OL neuropils'\n",
    "          , 'down_5_tresh': '# cells of type above 10% output connections in the 5 OL neuropils'\n",
    "          , 'pre_5_perc': 'Bonus: % presynaptic sites across all cells of type in the 5 OL neuropils'\n",
    "          , 'pre_5_tresh': 'Bonus: # cells of type above 10% presynaptic sites in the 5 OL neuropils'\n",
    "        })\\\n",
    "        .to_excel(out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu = pd.DataFrame()\n",
    "for area in ['VPN', 'VCN', 'other']:\n",
    "    neuron_list = olt.get_neuron_list(primary_classification=area, side='both')\n",
    "    df_neu = pd.concat([df_neu, get_neu_weights(neuron_list, area)])\n",
    "summarize(df_neu, 'issue_649_vpn-vcn-other.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed unused neurons\n",
    "\n",
    "There are a number of neurons that are in neuPrint, but not used in the analysis. Here we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_list = olt.get_neuron_list(side='both')\n",
    "\n",
    "cql = \"\"\"\n",
    "    MATCH (n:Neuron) \n",
    "    where not n.type is null\n",
    "    RETURN distinct n.type as type, n.instance as instance\n",
    "\"\"\"\n",
    "all_n = fetch_custom(cql)\n",
    "unused_list = all_n[~all_n['type'].isin(neuron_list['type'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unused = get_neu_weights(unused_list, 'unused')\n",
    "\n",
    "summarize(df_unused, 'issue_649_unused.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single cell type analysis\n",
    "\n",
    "The following cell helps looking at the synapse counts relevant for VPN or VCN cells. They show the percentage of total synapses for both hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_type = 'PVLP046'\n",
    "neuron_class = 'VPN'\n",
    "\n",
    "\n",
    "match neuron_class:\n",
    "    case 'VPN':\n",
    "        dir = 'pre'\n",
    "    case 'VCN':\n",
    "        dir = 'post'\n",
    "    case _:\n",
    "        dir = 'pre'\n",
    "\n",
    "cql = f\"\"\"\n",
    "    MATCH (n:Neuron)\n",
    "    WHERE n.type='{neuron_type}'\n",
    "    WITH apoc.convert.fromJsonMap(n.roiInfo) AS nri, n\n",
    "    WITH \n",
    "        COALESCE(nri['ME(R)']['{dir}'], 0) as me_wgt\n",
    "      , COALESCE(nri['LA(R)']['{dir}'], 0) as la_wgt\n",
    "      , COALESCE(nri['LO(R)']['{dir}'], 0) as lo_wgt\n",
    "      , COALESCE(nri['LOP(R)']['{dir}'], 0) as lop_wgt\n",
    "      , COALESCE(nri['AME(R)']['{dir}'], 0) as ame_wgt\n",
    "      , COALESCE(nri['OL(R)']['{dir}'], 0) as ol_wgt\n",
    "      , COALESCE(nri['ME(L)']['{dir}'], 0) as mel_wgt\n",
    "      , COALESCE(nri['LA(L)']['{dir}'], 0) as lal_wgt\n",
    "      , COALESCE(nri['LO(L)']['{dir}'], 0) as lol_wgt\n",
    "      , COALESCE(nri['LOP(L)']['{dir}'], 0) as lopl_wgt\n",
    "      , COALESCE(nri['AME(L)']['{dir}'], 0) as amel_wgt\n",
    "      , COALESCE(nri['OL(L)']['{dir}'], 0) as oll_wgt\n",
    "      , COALESCE(n.{dir}, 0) as wgt\n",
    "      , n\n",
    "    RETURN \n",
    "        n.bodyId AS bodyId\n",
    "      , n.instance AS instance\n",
    "      , '{neuron_class}' as class\n",
    "      , sum(la_wgt) as la_weight\n",
    "      , sum(me_wgt) as me_weight\n",
    "      , sum(lop_wgt) as lop_weight\n",
    "      , sum(lo_wgt) as lo_weight\n",
    "      , sum(ame_wgt) as ame_weight\n",
    "      , sum(ol_wgt) as ol_weight\n",
    "      , sum(lal_wgt) as lal_weight\n",
    "      , sum(mel_wgt) as mel_weight\n",
    "      , sum(lopl_wgt) as lopl_weight\n",
    "      , sum(lol_wgt) as lol_weight\n",
    "      , sum(amel_wgt) as amel_weight\n",
    "      , sum(oll_wgt) as oll_weight\n",
    "      , sum(wgt) as prel_weight\n",
    "      , toFloat(sum(la_wgt) + sum(me_wgt) + sum(lop_wgt) + sum(lo_wgt) + sum(ame_wgt) + sum(lal_wgt) + sum(mel_wgt) + sum(lopl_wgt) + sum(lol_wgt) + sum(amel_wgt)) / sum(wgt) as perc\n",
    "      , toFloat(sum(la_wgt) + sum(me_wgt) + sum(lop_wgt) + sum(lo_wgt) + sum(ame_wgt)) / sum(wgt) as perc_r\n",
    "      , toFloat(sum(lal_wgt) + sum(mel_wgt) + sum(lopl_wgt) + sum(lol_wgt) + sum(amel_wgt)) / sum(wgt) as perc_l\n",
    "      , toFloat(sum(ol_wgt)) / sum(wgt) as olr_perc\n",
    "      , toFloat(sum(ol_wgt) + sum(oll_wgt)) / sum(wgt) as ol_perc\n",
    "\"\"\"\n",
    "\n",
    "df_single = fetch_custom(cql)\n",
    "df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
